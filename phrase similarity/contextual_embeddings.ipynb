{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.layers import Input,BatchNormalization\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"PiC/phrase_similarity\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embedding(phrase, sentence):\n",
    "    # Tokenize and encode the sentence\n",
    "    inputs = tokenizer.encode_plus(sentence, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "\n",
    "    # Tokenize the phrase and find the corresponding token ids\n",
    "    phrase_tokens = tokenizer.tokenize(phrase)\n",
    "    phrase_ids = tokenizer.convert_tokens_to_ids(phrase_tokens)\n",
    "\n",
    "    start_index = None\n",
    "\n",
    "    # Locating phrase tokens\n",
    "    phrase_len = len(phrase_ids)\n",
    "    for i in range(len(input_ids) - phrase_len + 1):\n",
    "        if input_ids[i:i + phrase_len].tolist() == phrase_ids:\n",
    "            start_index = i\n",
    "            break\n",
    "\n",
    "    # Edge case when start index is not found\n",
    "    if start_index is None:\n",
    "        return torch.zeros(1, bert_model.config.hidden_size)\n",
    "\n",
    "    # Getting the BERT embeddings\n",
    "    outputs = bert_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Averaging the embeddings for the phrase tokens\n",
    "    phrase_embedding = embeddings[0, start_index:start_index + phrase_len, :].mean(dim=0)\n",
    "    return phrase_embedding.unsqueeze(0)\n",
    "\n",
    "\n",
    "def prepare_data(data_split):\n",
    "    X, Y = [], []\n",
    "    for item in data_split:\n",
    "        emb1 = get_embedding(item['phrase1'], item['sentence1'])\n",
    "        emb2 = get_embedding(item['phrase2'], item['sentence2'])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = cosine_similarity(emb1, emb2).reshape(-1, 1)\n",
    "\n",
    "        X.append(cos_sim.detach().numpy())\n",
    "        Y.append(item['label'])\n",
    "        print(item['idx'])\n",
    "\n",
    "    return np.array(X), np.array(Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "train_data = dataset['train'].shard(num_shards=1, index=0)\n",
    "val_data = dataset['validation'].shard(num_shards=1, index=0)\n",
    "test_data = dataset['test'].shard(num_shards=1, index=0)\n",
    "\n",
    "\n",
    "X_train, y_train = prepare_data(train_data)\n",
    "X_val, y_val = prepare_data(val_data)\n",
    "X_test, y_test = prepare_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    x = Dense(256, activation='relu')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Output layer with a single unit for binary classification\n",
    "    output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(1)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "precognlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
