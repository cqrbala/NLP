{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83615b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"paws\", \"labeled_final\")\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    embedding = embedding_model.encode(sentence, convert_to_tensor=True)\n",
    "    return embedding.numpy()\n",
    "\n",
    "def create_features(data):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    for item in data:\n",
    "        emb1 = get_sentence_embedding(item['sentence1'])\n",
    "        emb2 = get_sentence_embedding(item['sentence2'])\n",
    "        combined_embedding = np.concatenate([emb1, emb2])\n",
    "        embeddings.append(combined_embedding)\n",
    "        labels.append(item['label'])\n",
    "        print(item['id'])\n",
    "    return np.array(embeddings), np.array(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_model(bert_dim):\n",
    "    # Input for concatenated embeddings\n",
    "    input_embedding = Input(shape=(bert_dim * 2,), dtype='float32')\n",
    "\n",
    "    # Reshape to add a dummy sequence length dimension\n",
    "    reshaped_embedding = Reshape((1, bert_dim * 2))(input_embedding)\n",
    "\n",
    "    # Transformer block with 2 layers\n",
    "    for _ in range(2):\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=8, \n",
    "            key_dim=bert_dim // 8\n",
    "        )(reshaped_embedding, reshaped_embedding)\n",
    "\n",
    "        attention_output = LayerNormalization(epsilon=1e-6)(reshaped_embedding + attention_output)\n",
    "\n",
    "        ff_output = Dense(bert_dim * 4, activation='relu')(attention_output)\n",
    "        ff_output = Dropout(0.2)(ff_output) \n",
    "        ff_output = Dense(bert_dim * 2)(ff_output)\n",
    "\n",
    "        reshaped_embedding = LayerNormalization(epsilon=1e-6)(attention_output + ff_output)\n",
    "\n",
    "    # Global average pooling\n",
    "    pooled_output = GlobalAveragePooling1D()(reshaped_embedding)\n",
    "\n",
    "    x = Dense(512, activation='relu')(pooled_output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=input_embedding, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "train_data = dataset['train'].shard(num_shards=20, index=0)\n",
    "val_data = dataset['validation'].shard(num_shards=20, index=0)\n",
    "test_data = dataset['test'].shard(num_shards=20, index=0)\n",
    "\n",
    "# Create features\n",
    "train_embeddings, train_labels = create_features(train_data)\n",
    "val_embeddings, val_labels = create_features(val_data)\n",
    "test_embeddings, test_labels = create_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the model\n",
    "bert_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "combined_model = create_combined_model(bert_dim)\n",
    "combined_model.fit(train_embeddings, train_labels, epochs=100, batch_size=32, validation_data=(val_embeddings, val_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = combined_model.evaluate(test_embeddings, test_labels)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "precognlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
