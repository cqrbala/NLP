{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream dataset\n",
    "dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Lowercasing and removing special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text,re.I|re.A).lower()\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            processed_words.append(lemma)\n",
    "    return processed_words\n",
    "\n",
    "# Initialize model\n",
    "word2vec_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and train\n",
    "max_docs = 1000000\n",
    "initial_vocab_size = 5000  # Number of documents to build initial vocabulary\n",
    "update_vocab_size = 500    # Update vocabulary every 'update_vocab_size' documents\n",
    "\n",
    "processed_texts = []\n",
    "for i, doc in enumerate(dataset.take(max_docs)):\n",
    "    processed_text = preprocess(doc['text'])\n",
    "    processed_texts.append(processed_text)\n",
    "    if word2vec_model is None and len(processed_texts) >= initial_vocab_size:\n",
    "        word2vec_model = Word2Vec(processed_texts, vector_size=150, window=7, min_count=3, workers=4, epochs=20)\n",
    "        processed_texts = []\n",
    "    elif word2vec_model is not None and len(processed_texts) >= update_vocab_size:\n",
    "        word2vec_model.build_vocab(processed_texts, update=True)\n",
    "        word2vec_model.train(processed_texts, total_examples=len(processed_texts), epochs=word2vec_model.epochs)\n",
    "        processed_texts = []\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Processed {i} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Simlex 999 dataset\n",
    "simlex = pd.read_csv('./simlex999.csv')\n",
    "\n",
    "# Function to calculate similarity using the trained Word2Vec model\n",
    "def calculate_similarity(model, word1, word2):\n",
    "    if word1 in model.wv.key_to_index and word2 in model.wv.key_to_index:\n",
    "        return model.wv.similarity(word1, word2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Calculate similarities for each pair in Simlex 999\n",
    "simlex['predicted_score_word2vec'] = simlex.apply(lambda row: calculate_similarity(word2vec_model, row['word1'], row['word2'])*10, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_scores = simlex['sim_value']\n",
    "predicted_scores = simlex['predicted_score_word2vec']\n",
    "\n",
    "# Spearman's Correlation Coefficient\n",
    "spearman_corr, _ = spearmanr(actual_scores, predicted_scores)\n",
    "print(f\"Spearman's Correlation: {spearman_corr}\")\n",
    "\n",
    "# Pearson Correlation Coefficient\n",
    "pearson_corr, _ = pearsonr(actual_scores, predicted_scores)\n",
    "print(f\"Pearson Correlation: {pearson_corr}\")\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(actual_scores, predicted_scores)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = simlex['label1'].unique()\n",
    "spearman_corrs = []\n",
    "pearson_corrs = []\n",
    "\n",
    "# Category-wise Performance Analysis\n",
    "for category in simlex['label1'].unique():\n",
    "    category_data = simlex[simlex['label1'] == category]\n",
    "    actual_scores = category_data['sim_value']\n",
    "    predicted_scores = category_data['predicted_score_word2vec']\n",
    "\n",
    "    spearman_corr, _ = spearmanr(actual_scores, predicted_scores)\n",
    "    pearson_corr, _ = pearsonr(actual_scores, predicted_scores)\n",
    "    \n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Spearman's Correlation: {spearman_corr}\")\n",
    "    print(f\"Pearson Correlation: {pearson_corr}\")\n",
    "    print()\n",
    "\n",
    "    spearman_corrs.append(spearman_corr)\n",
    "    pearson_corrs.append(pearson_corr)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = range(len(categories))\n",
    "plt.bar(x, spearman_corrs, width=0.4, label='Spearman', align='center')\n",
    "plt.bar(x, pearson_corrs, width=0.4, label='Pearson', align='edge')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Category-wise Performance Analysis')\n",
    "plt.xticks(x, categories)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "precognlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
